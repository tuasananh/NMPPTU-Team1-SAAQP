\section{Ứng dụng trong học máy}
\label{sec:applications}
Phương pháp được đề xuất, giống như thuật toán GD, có nhiều ứng dụng trong học máy. Trong báo cáo này, nhóm chúng tôi trình bày và tái triển khai ba thực nghiệm ứng với ba ứng dụng tiêu biểu được trình bày trong bài báo \cite{thang2024saaqp}, bao gồm lựa chọn đặc trưng có giám sát (\textit{supervised feature selection}), hồi quy (\textit{regression}) và phân loại (\textit{classification}), để chứng minh độ chính xác và hiệu quả tính toán so với các phương pháp thay thế khác.

Đầu tiên, bài toán lựa chọn đặc trưng có thể được mô hình hóa như một bài toán cực tiểu hóa một hàm phân thức giả lồi trên một tập lồi, đây là một lớp con của Bài toán \ref{eq:OP}. Bài toán này được sử dụng để so sánh phương pháp tiếp cận được đề xuất với các phương pháp thần kinh động (\textit{neurodynamic approaches}). 

Thứ hai, vì bài toán hồi quy logistic đa biến là một bài toán quy hoạch lồi, nên thuật toán GDA và các biến thể có sẵn của thuật toán GD có thể được sử dụng để giải nó. 

Cuối cùng, một mô hình mạng nơ-ron cho bài toán phân loại ảnh cũng giống như một bài toán quy hoạch với hàm mục tiêu không lồi cũng không tựa lồi. Để huấn luyện mô hình này, các tác giả sử dụng biến thể ngẫu nhiên của phương pháp GDA (Thuật toán SGDA) như một kỹ thuật heuristic. Mặc dù sự hội tụ của thuật toán không thể được đảm bảo như trong các trường hợp hàm mục tiêu giả lồi và tựa lồi, nghiên cứu lý thuyết đã chỉ ra rằng nếu dãy điểm có một điểm giới hạn, nó sẽ hội tụ về một điểm dừng của bài toán (xem Định lý \ref{thm:convergence}). Các thực nghiệm tính toán chỉ ra rằng phương pháp được đề xuất vượt trội hơn so với các phương pháp thần kinh động và gradient descent hiện có.
Cuối cùng, một mô hình mạng nơ-ron cho bài toán phân loại ảnh cũng giống như một bài toán quy hoạch với hàm mục tiêu không lồi cũng không tựa lồi. Để huấn luyện mô hình này, các tác giả sử dụng biến thể ngẫu nhiên của phương pháp GDA (Thuật toán SGDA) như một kỹ thuật heuristic. Mặc dù sự hội tụ của thuật toán không thể được đảm bảo như trong các trường hợp hàm mục tiêu giả lồi và tựa lồi, nghiên cứu lý thuyết đã chỉ ra rằng nếu dãy điểm có một điểm giới hạn, nó sẽ hội tụ về một điểm dừng của bài toán (xem Định lý \ref{thm:convergence}). Các thực nghiệm tính toán chỉ ra rằng phương pháp được đề xuất vượt trội hơn so với các phương pháp thần kinh động và gradient descent hiện có.

\subsection*{5.1 Chọn đặc trưng có giám sát}
Bài toán chọn đặc trưng được thực hiện trên tập dữ liệu với tập gồm $p$ đặc trưng $\mathcal{F} = \{F_1, \dots, F_p\}$ và tập gồm $n$ mẫu $\{(x_i, y_i) \mid i = 1, \dots, n\}$, trong đó $x_i = (x_{i1}, \dots, x_{ip})^T$ là vectơ đặc trưng $p$ chiều của mẫu thứ $i$ và $y_i \in \{1, \dots, m\}$ đại diện cho các nhãn tương ứng chỉ ra các lớp hoặc giá trị mục tiêu. Trong \cite{wang2021neurodynamic}, một tập con tối ưu gồm $k$ đặc trưng $\{F_1, \dots, F_k\} \subseteq \mathcal{F}$ được chọn với độ dư thừa thấp nhất và độ liên quan cao nhất đến lớp mục tiêu $y$. Độ dư thừa của đặc trưng được đặc trưng bởi một ma trận bán xác định dương $Q$. Khi đó, mục tiêu đầu tiên là cực tiểu hóa hàm bậc hai lồi $w^T Q w$. Độ liên quan của đặc trưng được đo lường bởi $\rho^T w$, trong đó $\rho = (\rho_1, \dots, \rho_p)^T$ là một vectơ tham số liên quan. Do đó, mục tiêu thứ hai là cực đại hóa hàm tuyến tính $\rho^T w$. Kết hợp hai mục tiêu này, ta suy ra bài toán tương đương như sau:

\begin{equation}
   \label{bai toan : 14}
   \begin{aligned}
      & \underset{}{\text{min}}
      & & \frac{w^T Q w}{\rho^T w} \\
      & \text{v.đ.k}
      & & e^T w = 1 \\
      & & & w \geq 0,
   \end{aligned}
   \tag{14}
\end{equation}
trong đó $w = (w_1, \dots, w_p)^T$ là vectơ điểm số đặc trưng cần xác định. Vì hàm mục tiêu của bài toán (\ref{bai toan : 14}) có dạng phân thức, trong đó tử là một hàm lồi và mẫu là một hàm tuyến tính dương, nên nó giả lồi trên tập ràng buộc. Do đó, chúng ta có thể giải bài toán (\ref{bai toan : 14}) bằng Thuật toán GDA.

Trong thực nghiệm, bài báo \cite{thang2024saaqp} triển khai các thuật toán với tập dữ liệu Parkinsons, bao gồm 23 đặc trưng và 197 mẫu, được tải xuống tại \url{https://archive.ics.uci.edu/ml/datasets/parkinsons}. Ma trận hệ số tương đồng $Q$ được xác định bởi $Q = \delta I_p + S$ \cite{wang2021neurodynamic}, trong đó  $S = (s_{ij})$ là ma trận $p \times p$ với:

$$
s_{ij} = \max \left\{ 0, \frac{I(F_i; F_j; y)}{H(F_i) + H(F_j)} \right\},
$$
entropy thông tin của một vectơ biến ngẫu nhiên $\hat{X}$ là:
$$
H(\hat{X}) = -\sum_{\hat{x} \in \hat{X}} p(\hat{x}) \log p(\hat{x}),
$$
đa thông tin (multi-information) của ba vectơ ngẫu nhiên $\hat{X}, \hat{Y}, \hat{Z}$ là $I(\hat{X}; \hat{Y}; \hat{Z}) = I(\hat{X}; \hat{Y}) - I(\hat{X}; \hat{Y} \mid \hat{Z})$ với thông tin tương hỗ của hai vectơ ngẫu nhiên $\hat{X}, \hat{Y}$ được định nghĩa bởi:
$$
I(\hat{X}; \hat{Y}) = \sum_{\hat{x} \in \hat{X}} \sum_{\hat{y} \in \hat{Y}} p(\hat{x}, \hat{y}) \log \frac{p(\hat{x}, \hat{y})}{p(\hat{x}) p(\hat{y})}
$$
và thông tin tương hỗ có điều kiện giữa $\hat{X}, \hat{Y}$ và $\hat{Z}$ được định nghĩa bởi:
$$
I(\hat{X}; \hat{Y} \mid \hat{Z}) = \sum_{\hat{x} \in \hat{X}} \sum_{\hat{y} \in \hat{Y}} \sum_{\hat{z} \in \hat{Z}} p(\hat{x}, \hat{y}, \hat{z}) \log \frac{p(\hat{x}, \hat{y} \mid \hat{z})}{p(\hat{x} \mid \hat{z}) p(\hat{y} \mid \hat{z})}.
$$

Vectơ độ liên quan đặc trưng $\rho = (\rho_1, \dots, \rho_p)^T$ được xác định bởi điểm Fisher:

$$
\rho(F_i) = \frac{\sum_{j=1}^{K} n_j (\mu_{ij} - \mu_i)^2}{\sum_{j=1}^{K} n_j \sigma_{ij}^2},
$$
trong đó $n_j$ biểu thị số lượng mẫu trong lớp $j$, $\mu_{ij}$ biểu thị giá trị trung bình của đặc trưng $F_i$ cho các mẫu trong lớp $j$, $\mu_i$ là giá trị trung bình của đặc trưng $F_i$, và $\sigma_{ij}^2$ biểu thị giá trị phương sai của đặc trưng $F_i$ cho các mẫu trong lớp $j$. Bảng \ref{tab:comparing_scipy_gda} so sánh hiệu năng của thuật toán GDA với Scipy.

\begin{table}[H]
\caption{So sánh hiệu năng giữa thuật toán GDA và thư viện Scipy}
\label{tab:comparing_scipy_gda}
\centering
\setlength{\tabcolsep}{9pt} 
\small 
\begin{tabular}{l|lll|lll}
\hline
Seed 
& \multicolumn{3}{c|}{Thuật toán GDA (đề xuất)} 
& \multicolumn{3}{c}{Scipy} \\
 & $f(x^*)$ & \#Iter & Time (ms)
 & $f(x^*)$ & \#Iter & Time (ms) \\
\hline
1  & 0.152478 & 32 & 0.0186 & 0.152478 & 19 & 0.0062 \\
11 & 0.153480 & 51 & 0.0967 & 0.153480 & 15 & 0.0302 \\
21 & 0.153834 & 31 & 0.0160 & 0.153834 & 18 & 0.0062 \\
31 & 0.153501 & 55 & 0.0299 & 0.153501 & 16 & 0.0055 \\
41 & 0.154281 & 50 & 0.0260 & 0.154281 & 13 & 0.0040 \\
51 & 0.154185 & 60 & 0.0312 & 0.154185 & 15 & 0.0059 \\
61 & 0.154878 & 57 & 0.0343 & 0.154878 & 16 & 0.0062 \\
71 & 0.153465 & 33 & 0.0217 & 0.153465 & 16 & 0.0056 \\
81 & 0.153517 & 59 & 0.0318 & 0.153517 & 13 & 0.0035 \\
91 & 0.153269 & 52 & 0.0302 & 0.153269 & 15 & 0.0050 \\
\hline
\end{tabular}
\end{table}

\subsection*{5.2 Hồi quy logistic đa biến}
Các thực nghiệm được thực hiện với tập dữ liệu bao gồm $N$ quan sát $(\mathbf{a}_i, b_i) \in \mathbb{R}^d \times \mathbb{R}, i = 1, \dots, n$. Hàm mất mát cross-entropy cho hồi quy logistic đa biến được cho bởi $J(x) = -\sum_{i=1}^{N} (b_i \log(\sigma(-x^T \mathbf{a}_i)) + (1 - b_i) \log(1 - \sigma(-x^T \mathbf{a}_i)))$, trong đó $\sigma$ là hàm sigmoid. Kết hợp với điều chuẩn $\ell_2$, chúng ta có hàm mất mát được điều chuẩn $\bar{J}(x) = J(x) + \frac{1}{2N}\|x\|^2$. Hệ số Lipschitz $L$ được ước tính bởi $\frac{1}{2N}(\|A\|^2/2 + 1)$, trong đó $A = (a_1^T, \dots, a_n^T)^T$.
Trong bài báo \cite{thang2024saaqp}, các tác giả so sánh các thuật toán huấn luyện bài toán hồi quy logistic bằng cách sử dụng các tập dữ liệu Mushrooms và W8a (xem Malitsky và Mishchenko 2020) \cite{malitsky2020adaptive}. Phương pháp GDA được so sánh với thuật toán GD có kích thước bước là $1/L$ và phương pháp tăng tốc Nesterov. Chúng tôi đã tiến hành triển khai lại thí nghiệm, kết quả chi tiết được minh họa trong Hình \hyperref[fig:hinh3]{3} và \hyperref[fig:hinh4]{4}. Kết quả thu được hoàn toàn tương thích với kết quả đã trình bày trong bài báo \cite{thang2024saaqp}. Các hình này cho thấy rằng Thuật toán GDA vượt trội hơn Thuật toán GD và phương pháp tăng tốc Nesterov về giá trị hàm mục tiêu trong các lần lặp. Đặc biệt, Hình \hyperref[fig:hinh4]{4} thể hiện sự thay đổi của giá trị hàm mục tiêu theo các hệ số $\kappa$ khác nhau. Trong hình này, ký hiệu "GDA\_0.75" tương ứng với trường hợp $\kappa = 0.75$. Hình \hyperref[fig:hinh5]{5} và hình \hyperref{fig:hinh6}{6} minh họa sự thay đổi learning rate với tập dữ liệu Mushrooms và W8a tương ứng.

\begin{figure}[htbp]
   \centering
   \includegraphics[width = 0.8\linewidth]{images/fig3.png}
   \caption{Kết quả tính toán cho hồi quy logistic với tập dữ liệu Mushrooms}
   \label{fig:hinh3}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig4.png}
   \caption{Kết quả tính toán cho hồi quy logistic với tập dữ liệu W8a}
   \label{fig:hinh4}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig5.png}
   \caption{Sự thay đổi của learning rate với tập dữ liệu Mushroom}
   \label{fig:hinh5}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig6.png}
   \caption{Sự thay đổi của learning rate với tập dữ liệu W8a}
   \label{fig:hinh6}
\end{figure}

\subsection*{5.3 Mạng nơ-ron cho phân loại}
Để cung cấp một ví dụ về cách thuật toán đề xuất có thể được triển khai vào một mô hình huấn luyện mạng nơ-ron, các tác giả đã sử dụng các kiến trúc ResNet-18 tiêu chuẩn đã được triển khai trong PyTorch và huấn luyện chúng để phân loại các hình ảnh được lấy từ tập dữ liệu Cifar10 (được tải xuống tại \url{https://www.cs.toronto.edu/~kriz/cifar.html}), trong khi xem xét hàm mất mát cross-entropy. Trong các nghiên cứu với ResNet-18, các tác giả đã sử dụng các thiết lập mặc định của Adam cho các tham số của nó.

Để huấn luyện mô hình mạng nơ-ron này, bài báo \cite{thang2024saaqp} sử dụng biến thể ngẫu nhiên của phương pháp GDA (Thuật toán SGDA) để so sánh với các thuật toán Stochastic Gradient Descent (SGD). Độ chính xác kiểm thử và mức độ mất mát dữ liệu trong quá trình huấn luyện được hiển thị trong Hình \hyperref[fig:hinh6]{6} và \hyperref[fig:hinh7]{7}. 

Dựa vào các biểu đồ, ta thấy thuật toán SGDA tốt hơn SGD với learning rate cố định và hội tụ nhanh hơn hẳn hai thuật toán còn lại. Tuy nhiên, thời gian chạy mỗi epoch tốn gấp đôi do phải tính toán trước giá trị loss, dù điều này có thể tối ưu bằng cách sử dụng lại loss trước đó. 

Trong thực nghiệm này, SGDA cần 2 epoch warmup để ổn định. Nếu không có warmup, SGDA sẽ giảm learning rate xuống khoảng $0.015$ ngay từ epoch đầu tiên và không giảm nữa, khiến nó hoạt động như SGD với learning rate cố định fixed\_lr $= 0.01$. Theo chúng tôi, lý do là ở các epoch đầu, trọng số khởi tạo ngẫu nhiên làm loss biến động không ổn định, khiến điều kiện Armijo bị vi phạm liên tục và learning rate giảm nhanh. Để thực sự huấn luyện mô hình này hiệu quả, ta cần có 2 epoch để model ổn định (dùng SGD với learning rate cố định) và các epoch sau đó có thể áp dụng SGDA để thuật toán tự lựa chọn learning rate hiệu quả.

Dù kết quả cuối cùng của SGDA (test accuracy $94.21\%$) thấp hơn một chút so với SGD (test accuracy $94.67\%$) với multistep learning rate (giảm learning rate mỗi 30 epoch) , việc hội tụ sớm và khả năng tự thích nghi chứng minh SGDA hoạt động tốt trong huấn luyện các mô hình học sâu. Hình \hyperref[fig:hinh9]{9} và \hyperref[fig:hinh10]{10} lần lượt mô tả sự thay đổi của learning rate trong quá trình huấn luyện, và so sánh thời gian tính toán trung bình mỗi epoch.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig7.png}
   \caption{Độ chính xác kiểm thử qua các lần lặp của mô hình ResNet-18}
   \label{fig:hinh7}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig8.png}
   \caption{Mức độ mất mát dữ liệu trong quá trình huấn luyện qua các lần lặp của mô hình ResNet-18.}
   \label{fig:hinh8}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig9.png}
   \caption{Sự thay đổi của learning rate trong quá trình huấn luyện mô hình ResNet18.}
   \label{fig:hinh9}
\end{figure}

\begin{figure}[H]
   \centering
   \includegraphics[width=0.8\linewidth]{images/fig10.png}
   \caption{So sánh thời gian tính toán trung bình mỗi epoch (Epoch Time) trên mô hình ResNet18.}
   \label{fig:hinh10}
\end{figure}