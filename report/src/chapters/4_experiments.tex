\section{Thực nghiệm số}

Trong mục này, chúng tôi sử dụng hai ví dụ có sẵn và hai ví dụ quy mô lớn với số chiều biến thay đổi để kiểm chứng hiệu năng của phương pháp được đề xuất. Các thí nghiệm ứng dụng trong học máy sẽ được trình bày ở mục tiếp theo. Mã nguồn của thuật toán được công bố tại:
\begin{center}
\url{https://github.com/TranNgocThangHUST/AdaptiveGDforQCP}
\end{center}

Tất cả các thử nghiệm được tiến hành bằng Python trên máy MacBook Pro M1 (bộ xử lý 8 nhân 3.2GHz, RAM 8.00 GB).Tiêu chí dừng trong các thí nghiệm là "số vòng lặp không vượt quá \#Iter". Ký hiệu $x^{\*}$ là điểm giới hạn của dãy lặp $\{x_k\}$ và \textit{Time} là thời gian CPU của thuật toán GDA sử dụng tiêu chí dừng .

Các hàm mục tiêu và ràng buộc phi lồi được minh họa trong Ví dụ 1 và 2 lấy từ \citet{Liu2022one}. Ngoài ra, Ví dụ 2 phức tạp hơn Ví dụ 1 về dạng của các hàm. Việc triển khai Ví dụ 3 với hàm mục tiêu lồi và kích thước biến n thay đổi cho phép chúng tôi đánh giá phương pháp đề xuất so với Thuật toán GD. Ví dụ 3 được thực hiện với hàm mục tiêu giả lồi (pseudoconvex) và nhiều giá trị khác nhau của n để chúng tôi có thể ước lượng cách tiếp cận của mình so với Thuật toán RNN


Để so sánh với phương pháp thần kinh động học trong \citet{Liu2022one} chúng tôi xét Bài toán OP($f, C$) với tập ràng buộc được xác định cụ thể bởi $C = \{x \in \mathbb{R}^n \mid g(x) \leq 0,\; Ax = b\}$, trong đó
$g(x) := (g_1(x), g_2(x), \ldots, g_m(x))^T$, và $g_i : \mathbb{R}^n \to \mathbb{R},\; i = 1,\ldots,m$ là các hàm 
tựa lồi (quansi-convex) khả vi, ma trận $A \in \mathbb{R}^{p \times n}$ và 
$b = (b_1, b_2, \ldots, b_p)^T \in \mathbb{R}^p.$
Nhắc lại rằng, trong \citet{Liu2022one}, các tác giả đã giới thiệu hàm
\[
\Psi(s) = 
\left\{
\begin{array}{rl} 
1      & s > 0;  \\[6pt]
[0,1]  & s = 0;  \\[6pt]
0      & s < 0.
\end{array}
\right\}
\]


và

\[
P(x) = \sum_{i=1}^{m} \max \{0, g_i(x)\}.
\]

Thuật toán neurodynamic được trình bày trong \citet{Liu2022one} ,sử dụng mạng neural recurrent (RNN) dành cho việc giải Problem OP($f$, $C$) dưới dạng bài toán bao hàm vi phân như sau:

\[
\frac{d}{dt} x(t) \in -\epsilon(x(t)) \nabla f(x(t)) - \partial P(x(t)) - \partial \|Ax(t) - b\|_1 \quad \text{(RNN)}
\]
với thành phần hiệu chỉnh $c(x(t))$ là:

\[
c(x(t)) = \{\prod_{i=1}^{m+p} c_i(t) | c_i(t) \in 1 - \Psi(J_i(x(i))), \quad i = 1, 2, \dots, m+p \}
\]

với

\[
J(x) = (g_1(x), \dots, g_m(x), |A_1x - b_1|, \dots, |A_px - b_p|)^T,
\]

Thuật ngữ subgradient của $P(x)$ là

\[
\partial P(x) = 
\begin{cases} 
0, & x \in \operatorname{int}(X) \\
\sum_{i \in I_0(x)}[0,1] \nabla g_i(x), & x \in \operatorname{bd}(X) \\
\sum_{i \in I_+(x)} \nabla g_i(x) + \sum_{i \in I_0(x)} [0,1] \nabla g_i(x), & x \notin X,
\end{cases}
\]

với

\[
X = \{ x : g_i(x) \leq 0,\ i = 1, 2, \dots, m\},
\]

\[
I_+(x) = \{ i \in \{1, 2, \dots, m\} : g_i(x) > 0\},
\]

\[
I_0(x) = \{ i \in \{1, 2, \dots, m\} : g_i(x) = 0\},
\]

và thuật ngữ subgradient của $||Ax - b||_1$ là:

\[
\partial ||Ax - b||_1 = \sum_{i=1}^{p} (2 \Psi (A_i x - b_i) - 1) A_i^T,
\]

với $A_i \in \mathbb{R}^{1 \times n}$ ($i = 1, 2, \dots, p$) là các vector hàng của ma trận $A$.

\subsection*{Ví dụ 1}

Xét bài toán tối ưu không lồi (OP(f,C):
\[
\begin{array}{ll}
\min & f(x) = \dfrac{x_1^2 + x_2^2 + 3}{1 + 2x_1 + 8x_2} \\[10pt]
\text{subject to} & x \in C,
\end{array}
\]

\begin{figure}[h]  % [h] = here (ở đây), có thể thay bằng [htbp] để linh hoạt vị trí
\centering
\includegraphics[width=0.8\textwidth]{src/images/Fig1.png} 
\caption{ Kết quả tính toán của Ví dụ 1} 
\label{fig:hinh1} 
\end{figure}

Với $C = \{ x = (x_1, x_2)^\top \in \mathbb{R}^2 \mid g(x) = -x_1^2 - 2x_1 x_2 \leq -4;\ x_1, x_2 \geq 0 \}$. . Có thể dễ dàng thấy rằng với bài toán này, hàm mục tiêu $f$ là giả lồi (pseudoconvex) trên tập khả thi lồi (Ví dụ 5.2 trong \citet{Liu2022}).

Hình\ref{fig:hinh1} minh họa các nghiệm tạm thời của phương pháp đề xuất đối với nhiều nghiệm khởi tạo ban đầu khác nhau. Kết quả cho thấy các nghiệm đều hội tụ về nghiệm tối ưu $x^* = (0.8922,\ 1.7957)$ của bài toán đã cho. Giá trị hàm mục tiêu do thuật toán GDA tạo ra là $0.4094$, tốt hơn so với giá trị tối ưu $0.4101$ của mạng neural trong \citet{Liu2022}.

\subsection*{Ví dụ 2}
Xét bài toán tối ưu giả lồi không trơn với các ràng buộc bất đẳng thức không lồi sau (Ví dụ 5.1 trong \citet{Liu2022one}):


\[
\begin{array}{ll}
\displaystyle \text{minimize}  & f(x) = \dfrac{e^{|x_2 - 3|} - 30}{x_1^2 + x_3^2 + 2x_4^2 + 4} \\[15pt]
\text{subject to}   & g_1(x) = (x_1 + x_3)^2 + 2x_4^2 \leq 10, \\
                    & g_2(x) = (x_2 - 1)^2 \leq 1, \\
                    & 2x_1 + 4x_2 + x_3 = -1,
\end{array}
\]

trong đó $x = (x_1, x_2, x_3, x_4)^\top \in \mathbb{R}^4$. . Hàm mục tiêu $f(x)$ là giả lồi không trơn trên miền khả thi $C$, và ràng buộc bất đẳng thức $g_1$ là liên tục và chuẩn lồi (quasiconvex) trên $C$,nhưng không phải giả lồi (Ví dụ 5.1 trong \citet{Liu2022one}). Có thể dễ dàng kiểm chứng rằng $x_2 \neq 3$ với mọi $x \in C$. Do đó, vector gradient của $|x_2 - 3|$ là $(x_2 - 3)/|x_2 - 3|$ với mọi $x \in C$. Từ đó, ta có thể xác định vector gradient của $f(x)$ tại điểm $x \in C$ để sử dụng trong thuật toán. \ref{fig:hinh2} cho thấy thuật toán GDA hội tụ về nghiệm tối ưu $x^* = (-1.0649,\ 0.4160,\ -0.5343,\ 0.0002)^\top$ với giá trị tối ưu $-3.0908$, tốt hơn so với giá trị tối ưu $-3.0849$ của mô hình mạng neural trong \citet{Liu2022one}.

\begin{figure}[h]  % [h] = here (ở đây), có thể thay bằng [htbp] để linh hoạt vị trí
\centering
\includegraphics[width=0.8\textwidth]{src/images/Fig2.png} 
\caption{ Kết quả tính toán của Ví dụ 2} 
\label{fig:hinh2} 
\end{figure}

\subsection*{Ví dụ 3}
Cho $e = (1, \dots, n) \in \mathbb{R}^n$ là vector toàn 1, $\alpha > 0$ và $\beta > 0$ là các hằng số thỏa mãn điều kiện tham số $2\alpha \geq 3\beta^{3/2}\sqrt{n}$. Xét bài toán OP($f, C$) (Ví dụ 4.5 trong \citet{ferreira2022frankwolfe}) với hàm mục tiêu
$$f(x) := \alpha^T x + \alpha x^T x + \frac{\beta}{\sqrt{1 + \beta x^T x}} e^T x,$$
trong đó $a \in \mathbb{R}_{++}^n$ là lồi và ràng buộc không lồi được cho bởi
$$C := \{ x \in \mathbb{R}_{++}^n \mid 1 \leq x_1 \dots x_n \}.$$
Phần này nhằm so sánh thuật toán Gradient Descent tăng tốc (GD) mà chúng tôi đề xuất với thuật toán GD cổ điển. Chúng tôi chọn ngẫu nhiên $\beta = 0.741271$, $\alpha = 3\beta^{3/2}\sqrt{n} + 1$ để thỏa mãn điều kiện tham số và hệ số Lipschitz $L = (4\beta^{3/2}\sqrt{n} + 3\alpha)$ được gợi ý trong \citet{ferreira2022frankwolfe}. Bước kích thước của thuật toán GD là $x_0 = 5/L$. Bảng \ref{tab:example3} trình bày giá trị hàm mục tiêu tối ưu, số vòng lặp, thời gian tính toán của hai thuật toán trong các chiều khác nhau. Từ kết quả này, thuật toán GDA cho hiệu suất tốt hơn thuật toán GD cổ điển ở cả thời gian tính toán và giá trị đầu ra tối ưu, đặc biệt với các bài toán có chiều lớn.

\begin{table}[htbp]
\centering
\caption{Computational results for Example 3}
\label{tab:example3}
\begin{tabular}{r|rrr|rrr}
\toprule
 & \multicolumn{3}{c|}{Algorithm GDA (proposed)} & \multicolumn{3}{c}{Algorithm GD} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
$n$ & $f(x^*)$ & \#Iter & Time & $f(x^*)$ & \#Iter & Time \\
\midrule
10   & 79.3264  & 9   & 0.9576 & 79.3264  & 15 & 1.5463 \\
20   & 220.5622 & 10  & 6.0961 & 220.5622 & 67 & 34.0349 \\
50   & 857.1166 & 12  & 2.8783 & 857.1166 & 16 & 4.6824 \\
100  & 2392.5706& 12  & 17.2367& 2392.5706& 17 & 30.8886 \\
200  & 7065.9134& 65  & 525.1199& 7179.3542& 200& 1610.6560 \\
500  & 26,877.7067& 75& 2273.0011& 27,145.6292& 500& 14,113.5003 \\
\bottomrule
\end{tabular}
\end{table}

\vspace{2em}
\begin{table}[htbp]
\centering
\caption{Computational results for Example 4}
\label{tab:example4}
\begin{tabular}{r|rrr|rrr}
\toprule
 & \multicolumn{3}{c|}{Algorithm GDA (proposed)} & \multicolumn{3}{c}{Algorithm RNN} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
$n$ & $-\ln(-f(x^*))$ & \#Iter & Time & $-\ln(-f(x^*))$ & \#Iter & Time \\
\midrule
10   & 5.1200 & 10 & 0.3  & 5.1506 & 1000 & 256  \\
20   & 2.5600 & 10 & 0.8  & 2.5673 & 1000 & 503  \\
50   & 1.0240 & 10 & 2    & 1.0299 & 1000 & 832  \\
100  & 0.5125 & 10 & 7    & 13.7067& 1000 & 1420 \\
300  & 15.7154& 10 & 84   & 39.3080& 1000 & 3292 \\
400  & 20.9834& 10 & 163  & 57.6837& 1000 & 4426 \\
600  & 29.9228& 10 & 371  & 83.6265& 1000 & 6788 \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Ví dụ 4}

Để so sánh thuật toán GDA với thuật toán RNN trong \citet{Liu2022one}, chúng tôi xét bài toán OP($f, C$) với hàm mục tiêu
$$f(x) = -\exp\left( -\sum_{i=1}^{n} \frac{x_i^2}{\theta_i^2} \right),$$
hàm này là giả lồi trên tập ràng buộc lồi
$$C := \{ Ax = b,\ g(x) \leq 0 \},$$
trong đó $x \in \mathbb{R}^n$, vector tham số $\theta = (\theta_1, \theta_2, \dots, \theta_n)^\top$ với $\theta_i > 0$, ma trận $A = (a_1, a_2, \dots, a_n) \in \mathbb{R}^{1\times n}$ với $a_i = 1$ nếu $1 \leq i \leq n/2$ và $a_i = 3$ nếu $n/2 < i \leq n$, và hằng số $b = 16$. Các ràng buộc bất đẳng thức là
\[g_i(x) = x_{10(i-1)+1}^2 + x_{10(i-1)+2}^2 + \cdots + x_{10(i-1)+10}^2 - 20,\]
với $i = 1, 2, \dots, \lfloor n/10 \rfloor$. 
Bảng \ref{tab:example4} trình bày kết quả tính toán của thuật toán GDA và RNN. Lưu ý rằng hàm $-\ln(-z)$ là đơn điệu tăng theo $z \in \mathbb{R}$, $z < 0$. Do đó, để so sánh giá trị tối ưu gần đúng qua các vòng lặp, chúng tôi tính $-\ln(-f(x^*))$ thay vì $f(x^*)$, với nghiệm tối ưu gần đúng $x^*$. 
Với mỗi $n$, chúng tôi giải bài toán OP($f, C$) để tìm giá trị $-\ln(-f(x^{*}))$, số vòng lặp (\#Iter) để dừng thuật toán, và thời gian tính toán (Time) tính bằng giây. Kết quả tính toán cho thấy thuật toán đề xuất vượt trội hơn thuật toán RNN trong các kịch bản thử nghiệm về cả giá trị tối ưu và thời gian tính toán, đặc biệt khi chiều không gian lớn.
